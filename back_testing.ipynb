{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import select\n",
    "from scipy.stats import poisson\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from data_manager import DataManager\n",
    "from models import TradPlayerStats, Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson_prob_above_threshold(rolling_mean, threshold):\n",
    "    cdf = poisson.cdf(threshold, rolling_mean)\n",
    "    prob_above_threshold = 1 - cdf\n",
    "    return prob_above_threshold\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    return accuracy, precision, recall, f1, conf_matrix\n",
    "\n",
    "def train_and_evaluate(df, model):\n",
    "    X = df[['pts']]\n",
    "    y = df['y']\n",
    "    print(X)\n",
    "    print(y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.where(y_pred > 0.5, 1, 0)  # Threshold for binary classification\n",
    "    \n",
    "    return evaluate_model(y_test, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DataManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = dm.get_engine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "player_id = dm.get_player_id(\"Stephen Curry\")\n",
    "decision_threshold = 0.52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "stmt = (\n",
    "    select(TradPlayerStats, Game.date, Game.season, Game.season_type)\n",
    "    .join(Game, TradPlayerStats.game_id == Game.id)\n",
    "    .where(TradPlayerStats.player_id == player_id)\n",
    "    .order_by(Game.date)\n",
    ")\n",
    "\n",
    "data = pd.read_sql(stmt, engine)\n",
    "dfs = {season: df_season for season, df_season in data.groupby('season')}\n",
    "results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    pts\n",
      "0    10\n",
      "1    38\n",
      "2    22\n",
      "3    18\n",
      "4     5\n",
      "..  ...\n",
      "72   13\n",
      "73   31\n",
      "74   24\n",
      "75   30\n",
      "76   47\n",
      "\n",
      "[77 rows x 1 columns]\n",
      "0     38.0\n",
      "1     22.0\n",
      "2     18.0\n",
      "3      5.0\n",
      "4     22.0\n",
      "      ... \n",
      "72    31.0\n",
      "73    24.0\n",
      "74    30.0\n",
      "75    47.0\n",
      "76    32.0\n",
      "Name: y, Length: 77, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rusta\\AppData\\Local\\Temp\\ipykernel_24568\\1472544957.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['y'] = df['pts'].shift(-1)\n",
      "C:\\Users\\rusta\\AppData\\Local\\Temp\\ipykernel_24568\\1472544957.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['y'] = df['y']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[720], line 45\u001b[0m\n\u001b[0;32m     43\u001b[0m     dfs[season] \u001b[38;5;241m=\u001b[39m df \n\u001b[0;32m     44\u001b[0m     lr_model \u001b[38;5;241m=\u001b[39m LinearRegression()\n\u001b[1;32m---> 45\u001b[0m     accuracy, precision, recall, f1, conf_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseason\u001b[39m\u001b[38;5;124m'\u001b[39m: season,\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLinear Regression\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfusion_matrix\u001b[39m\u001b[38;5;124m'\u001b[39m: conf_matrix\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     54\u001b[0m     })\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Convert results to DataFrame for display\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[715], line 26\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(df, model)\u001b[0m\n\u001b[0;32m     23\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     24\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(y_pred \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Threshold for binary classification\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[715], line 8\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_model\u001b[39m(y_true, y_pred):\n\u001b[0;32m      7\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_true, y_pred)\n\u001b[1;32m----> 8\u001b[0m     precision \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     recall \u001b[38;5;241m=\u001b[39m recall_score(y_true, y_pred, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     10\u001b[0m     f1 \u001b[38;5;241m=\u001b[39m f1_score(y_true, y_pred, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rusta\\.virtualenvs\\nba_01-7yhASJUA\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\rusta\\.virtualenvs\\nba_01-7yhASJUA\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2190\u001b[0m, in \u001b[0;36mprecision_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   2023\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m   2024\u001b[0m     {\n\u001b[0;32m   2025\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2050\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2051\u001b[0m ):\n\u001b[0;32m   2052\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the precision.\u001b[39;00m\n\u001b[0;32m   2053\u001b[0m \n\u001b[0;32m   2054\u001b[0m \u001b[38;5;124;03m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2188\u001b[0m \u001b[38;5;124;03m    array([0.5, 1. , 1. ])\u001b[39;00m\n\u001b[0;32m   2189\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2190\u001b[0m     p, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2191\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2192\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2195\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprecision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2197\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2199\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[1;32mc:\\Users\\rusta\\.virtualenvs\\nba_01-7yhASJUA\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rusta\\.virtualenvs\\nba_01-7yhASJUA\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1775\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1612\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[0;32m   1613\u001b[0m \n\u001b[0;32m   1614\u001b[0m \u001b[38;5;124;03mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;124;03m array([2, 2, 2]))\u001b[39;00m\n\u001b[0;32m   1773\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m _check_zero_division(zero_division)\n\u001b[1;32m-> 1775\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1777\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1778\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rusta\\.virtualenvs\\nba_01-7yhASJUA\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1564\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1562\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1563\u001b[0m             average_options\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1564\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1565\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget is \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m but average=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1566\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoose another average setting, one of \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (y_type, average_options)\n\u001b[0;32m   1567\u001b[0m         )\n\u001b[0;32m   1568\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pos_label \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1569\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1570\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNote that pos_label (set to \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) is ignored when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1571\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage != \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m). You may use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1574\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1575\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']."
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def balance_dataframe_undersample(df, target_column):\n",
    "    class_counts = df[target_column].value_counts()\n",
    "    if len(class_counts) < 2:\n",
    "        print(f\"Skipping balancing for DataFrame due to only one class present\")\n",
    "        return df\n",
    "\n",
    "    # Identify the majority and minority classes\n",
    "    majority_class = class_counts.idxmax()\n",
    "    minority_class = class_counts.idxmin()\n",
    "\n",
    "    df_majority = df[df[target_column] == majority_class]\n",
    "    df_minority = df[df[target_column] == minority_class]\n",
    "\n",
    "    # If either class is empty, return the original DataFrame\n",
    "    if len(df_majority) == 0 or len(df_minority) == 0:\n",
    "        print(f\"Skipping balancing for DataFrame due to no instances of one class: {len(df_majority)} majority, {len(df_minority)} minority\")\n",
    "        return df\n",
    "\n",
    "    # Undersample the majority class to match the minority class size only if majority class is larger\n",
    "    difference = len(df_majority) - len(df_minority)\n",
    "    df_majority = df_majority.sample(frac=1)  # Shuffle\n",
    "    print(len(df_majority))\n",
    "    df_majority = df_majority.head(len(df_majority) - difference).reset_index(drop=True)\n",
    "    print(len(df_majority))\n",
    "    df_balanced = pd.concat([df_majority, df_minority], axis=0).reset_index(drop=True)\n",
    "    print(df_balanced['positive_true'].value_counts())\n",
    "    return df_balanced\n",
    "\n",
    "\n",
    "\n",
    "# Balance each DataFrame in the dictionary and calculate metrics\n",
    "for season, df in dfs.items():\n",
    "    df = df[['pts', 'date']]\n",
    "    threshold =  df.pts.median() + 2\n",
    "    df['y'] = df['pts'].shift(-1)\n",
    "    df['y'] = df['y']\n",
    "    df = df.dropna()\n",
    "    dfs[season] = df \n",
    "    lr_model = LinearRegression()\n",
    "    accuracy, precision, recall, f1, conf_matrix = train_and_evaluate(df, lr_model)\n",
    "    results.append({\n",
    "    'season': season,\n",
    "    'model': 'Linear Regression',\n",
    "    'accuracy': accuracy,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1_score': f1,\n",
    "    'confusion_matrix': conf_matrix.tolist()\n",
    "    })\n",
    "\n",
    "\n",
    "# Convert results to DataFrame for display\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>confusion_matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-14</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>[[27, 10], [22, 9]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-15</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>[[42, 5], [19, 4]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-16</td>\n",
       "      <td>0.623188</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>[[35, 5], [21, 8]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-17</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>[[41, 4], [23, 1]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-18</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>[[17, 6], [11, 7]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-19</td>\n",
       "      <td>0.593220</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>[[33, 6], [18, 2]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-21</td>\n",
       "      <td>0.547170</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>[[23, 7], [17, 6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-22</td>\n",
       "      <td>0.537037</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.390244</td>\n",
       "      <td>[[21, 9], [16, 8]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022-23</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>[[23, 6], [14, 3]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-24</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>[[35, 6], [18, 5]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     season  accuracy  precision    recall  f1_score     confusion_matrix\n",
       "0   2013-14  0.529412   0.473684  0.290323  0.360000  [[27, 10], [22, 9]]\n",
       "1   2014-15  0.657143   0.444444  0.173913  0.250000   [[42, 5], [19, 4]]\n",
       "2   2015-16  0.623188   0.615385  0.275862  0.380952   [[35, 5], [21, 8]]\n",
       "3   2016-17  0.608696   0.200000  0.041667  0.068966   [[41, 4], [23, 1]]\n",
       "4   2017-18  0.585366   0.538462  0.388889  0.451613   [[17, 6], [11, 7]]\n",
       "5   2018-19  0.593220   0.250000  0.100000  0.142857   [[33, 6], [18, 2]]\n",
       "6   2019-20       NaN   0.000000  0.000000  0.000000                   []\n",
       "7   2020-21  0.547170   0.461538  0.260870  0.333333   [[23, 7], [17, 6]]\n",
       "8   2021-22  0.537037   0.470588  0.333333  0.390244   [[21, 9], [16, 8]]\n",
       "9   2022-23  0.565217   0.333333  0.176471  0.230769   [[23, 6], [14, 3]]\n",
       "10  2023-24  0.625000   0.454545  0.217391  0.294118   [[35, 6], [18, 5]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nba_01-7yhASJUA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
